{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import h5py\n",
    "#import pywt\n",
    "\n",
    "%pip install pingouin\n",
    "\n",
    "import pingouin as pg\n",
    "%pip install matplotlib -U\n",
    "%pip install Pillow -U\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = './datasets/all_data'\n",
    "\n",
    "paths = [all_data + '/feeding/csv/Feeding_25Hz_',\n",
    "         all_data + '/swimming/csv/Swimming_25Hz_',\n",
    "         all_data + '/resting/csv/Resting_25Hz_',\n",
    "         all_data + '/ndm/csv/NDM_25Hz_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load datasets into pandas\n",
    "'''\n",
    "dfs = []\n",
    "\n",
    "for i in range(7):\n",
    "    df = pd.concat((pd.read_csv(path + str(i+1) + '.csv',\n",
    "                                index_col=['Date_Time'],\n",
    "                                parse_dates=['Date_Time'],\n",
    "                                infer_datetime_format=True) for path in paths), ignore_index=False, sort=False).iloc[:, 1:9]\n",
    "\n",
    "    \n",
    "    df = df.replace(to_replace={\"Non directed motion\": \"NDM\"})\n",
    "    \n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_stats = pd.DataFrame()\n",
    "dynamic_stats = pd.DataFrame()\n",
    "\n",
    "static_norms = pd.DataFrame()\n",
    "dynamic_norms = pd.DataFrame()\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    static_stats['Exp ' + str(i+1)] = pd.concat([df[['X_static', 'Y_static', 'Z_static']].max(),\n",
    "                                                 df[['X_static', 'Y_static', 'Z_static']].min(),\n",
    "                                                 df[['X_static', 'Y_static', 'Z_static']].mean(),\n",
    "                                                 df[['X_static', 'Y_static', 'Z_static']].std()], keys=['Max', 'Min', 'Mean', 'Std'])\n",
    "    \n",
    "    dynamic_stats['Exp ' + str(i+1)] = pd.concat([df[['X_dynamic', 'Y_dynamic', 'Z_dynamic']].max(),\n",
    "                                                  df[['X_dynamic', 'Y_dynamic', 'Z_dynamic']].min(),\n",
    "                                                  df[['X_dynamic', 'Y_dynamic', 'Z_dynamic']].mean(),\n",
    "                                                  df[['X_dynamic', 'Y_dynamic', 'Z_dynamic']].std()], keys=['Max', 'Min', 'Mean', 'Std'])\n",
    "\n",
    "    static_norms['Exp ' + str(i+1)] = pd.concat([pd.Series(np.linalg.norm(df[['X_static', 'Y_static', 'Z_static']].max())),\n",
    "                                                 pd.Series(np.linalg.norm(df[['X_static', 'Y_static', 'Z_static']].min())),\n",
    "                                                 pd.Series(np.linalg.norm(df[['X_static', 'Y_static', 'Z_static']].mean())),\n",
    "                                                 pd.Series(np.linalg.norm(df[['X_static', 'Y_static', 'Z_static']].std()))], keys=['Max', 'Min', 'Mean', 'Std'])\n",
    "    \n",
    "    dynamic_norms['Exp ' + str(i+1)] = pd.concat([pd.Series(np.linalg.norm(df[['X_dynamic', 'Y_dynamic', 'Z_dynamic']].max())),\n",
    "                                                  pd.Series(np.linalg.norm(df[['X_dynamic', 'Y_dynamic', 'Z_dynamic']].min())),\n",
    "                                                  pd.Series(np.linalg.norm(df[['X_dynamic', 'Y_dynamic', 'Z_dynamic']].mean())),\n",
    "                                                  pd.Series(np.linalg.norm(df[['X_dynamic', 'Y_dynamic', 'Z_dynamic']].std()))], keys=['Max', 'Min', 'Mean', 'Std'])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Estimation for ODBA Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kde_plot(data_df, feature='ODBA', log_scale=False):\n",
    "    label_list = data_df['Label'].unique().tolist()\n",
    "    for label in label_list:\n",
    "        class_data = data_df.loc[data_df['Label'] == label][feature]\n",
    "\n",
    "        # Draw the density plot for original data\n",
    "        ax = sns.kdeplot(data=class_data, legend=True, log_scale=log_scale)\n",
    "\n",
    "    ax.legend(label_list)\n",
    "\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_plot(pd.concat(dfs), feature='X_static')\n",
    "kde_plot(pd.concat(dfs), feature='Y_static')\n",
    "kde_plot(pd.concat(dfs), feature='Z_static')\n",
    "kde_plot(pd.concat(dfs), feature='X_dynamic', log_scale=True)\n",
    "kde_plot(pd.concat(dfs), feature='Y_dynamic', log_scale=True)\n",
    "kde_plot(pd.concat(dfs), feature='Z_dynamic', log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, df in enumerate(dfs):\n",
    "    print(\"Normality test for Exp \" + str(i+1))\n",
    "    f_data = df.loc[df['Label'] == 'NDM']['X_dynamic']\n",
    "#     f_data = df[['ODBA']]\n",
    "#     log_f = np.log10(f_data)\n",
    "    \n",
    "#     plt.hist(f_data, bins='auto')\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.hist(log_f, bins='auto')\n",
    "#     plt.show()\n",
    "\n",
    "#     sm.qqplot(f_data, line ='r')\n",
    "#     plt.show()\n",
    "    \n",
    "#     sm.qqplot(log_f, line ='r')\n",
    "#     plt.show()\n",
    "    \n",
    "    print(scipy.stats.normaltest(f_data))\n",
    "#     print(scipy.stats.normaltest(log_f))\n",
    "    \n",
    "#     print(pg.normality(f_data, alpha=0.05))\n",
    "#     print(pg.normality(log_f, alpha=0.05))\n",
    "\n",
    "#     rand_idx = np.random.randint(0,len(df)-5000)\n",
    "#     print(pg.normality(log_f[rand_idx:rand_idx+5000]), alpha=0.05)\n",
    "    \n",
    "#     print(pg.multivariate_normality(df[['X_dynamic', \n",
    "#                                         'Y_dynamic', \n",
    "#                                         'Z_dynamic']][rand_idx:rand_idx+5000], alpha=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, cov, n = [4, 5], [(0.6, 0.4), (0.4, 0.6)], 5000\n",
    "x, y = np.random.multivariate_normal(mean, cov, n).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pg.normality(x))\n",
    "print(pg.normality(y))\n",
    "print(pg.multivariate_normality(np.column_stack((x, y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(x, bins='auto')\n",
    "# plt.show()\n",
    "# plt.hist(y, bins='auto')\n",
    "# plt.show()\n",
    "\n",
    "plt.hist2d(x, y, bins=(69, 69), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install statsmodels\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(y, line ='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: 1, 2, 3, 4, 7\n",
    "# Val: 6\n",
    "# Test: 5\n",
    "train_df = pd.concat([dfs[0], dfs[1], dfs[2], dfs[3], dfs[6]])\n",
    "val_df = dfs[5]\n",
    "test_df = dfs[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']\n",
    "\n",
    "train_data = train_df[features + ['Label']]\n",
    "val_data = val_df[features + ['Label']]\n",
    "test_data = test_df[features + ['Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([df[features].isna().values.any() for df in dfs])\n",
    "print(train_data[features].isna().values.any())\n",
    "print(val_data[features].isna().values.any())\n",
    "print(test_data[features].isna().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in features:\n",
    "    mean = np.mean(train_data[column])\n",
    "    std  = np.std(train_data[column])\n",
    "    \n",
    "    train_data[column] = train_data[column].map(lambda x: (x-mean)/std)\n",
    "    val_data[column]   = val_data[column].map(lambda x: (x-mean)/std)\n",
    "    test_data[column]  = test_data[column].map(lambda x: (x-mean)/std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(train_data[features]))\n",
    "print(np.std(train_data[features]))\n",
    "\n",
    "print(np.mean(val_data[features]))\n",
    "print(np.std(val_data[features]))\n",
    "\n",
    "print(np.mean(test_data[features]))\n",
    "print(np.std(test_data[features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_plot(train_data, feature='X_static')\n",
    "kde_plot(train_data, feature='Y_static')\n",
    "kde_plot(train_data, feature='Z_static')\n",
    "kde_plot(train_data, feature='X_dynamic', log_scale=True)\n",
    "kde_plot(train_data, feature='Y_dynamic', log_scale=True)\n",
    "kde_plot(train_data, feature='Z_dynamic', log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_plot(val_data, feature='X_static')\n",
    "kde_plot(val_data, feature='Y_static')\n",
    "kde_plot(val_data, feature='Z_static')\n",
    "kde_plot(val_data, feature='X_dynamic', log_scale=True)\n",
    "kde_plot(val_data, feature='Y_dynamic', log_scale=True)\n",
    "kde_plot(val_data, feature='Z_dynamic', log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_plot(test_data, feature='X_static')\n",
    "kde_plot(test_data, feature='Y_static')\n",
    "kde_plot(test_data, feature='Z_static')\n",
    "kde_plot(test_data, feature='X_dynamic', log_scale=True)\n",
    "kde_plot(test_data, feature='Y_dynamic', log_scale=True)\n",
    "kde_plot(test_data, feature='Z_dynamic', log_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group contiguous time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_times(df):\n",
    "    time_diff = df.index.to_series().diff()\n",
    "    breaks = time_diff > pd.Timedelta('1s')\n",
    "    groups = breaks.cumsum()\n",
    "    \n",
    "    df['Group'] = groups\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = group_times(train_data)\n",
    "val_data = group_times(val_data)\n",
    "test_data = group_times(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_group = train_data['Group'].max()+1\n",
    "\n",
    "groups = [train_data[train_data['Group'] == i] for i in range(max_group)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for i, group in enumerate(groups):\n",
    "    seq_dict = {}\n",
    "    \n",
    "    for label in label_list:\n",
    "        seq_dict[label] = len(group[group['Label'] == label])\n",
    "    \n",
    "    counts.append(seq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(counts))\n",
    "\n",
    "for count_dict in counts:\n",
    "    print(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['Feeding', 'Swimming', 'Resting', 'NDM']\n",
    "\n",
    "for group in groups:\n",
    "    for label in label_list:\n",
    "        group = group[group['Label'] == label]\n",
    "        \n",
    "        if()\n",
    "        \n",
    "        if len(group) <= 50:\n",
    "            print(label + \": \" + str(group['Group'].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "i = 0\n",
    "while(len(data) == 0):\n",
    "    i += 1\n",
    "    chunk_idx = np.random.randint((train_data['Group'].max()+1))\n",
    "\n",
    "    data = train_data.loc[(train_data['Label'] == 'Feeding') & (train_data['Group'] == chunk_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequences(df, num_samples=None, seq_len=50, dims=1, train=True):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    label_list = ['Feeding', 'Swimming', 'Resting', 'NDM']\n",
    "    \n",
    "    for idx, label in enumerate(label_list):\n",
    "        print(str(idx) + \": \" + label)\n",
    "        \n",
    "        class_df = \n",
    "        if train:\n",
    "            X_class = np.zeros((num_samples, seq_len, dims), dtype=np.float32)\n",
    "            Y_class = np.full((num_samples, 1), idx, dtype=np.int64)\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                data = []\n",
    "                while(len(data) == 0):\n",
    "                    chunk_idx = np.random.randint((df['Group'].max()+1))\n",
    "                    \n",
    "                    data = df.loc[(df['Label'] == label) & (df['Group'] == chunk_idx)][features].to_numpy()\n",
    "                \n",
    "                rand = np.random.randint(len(data)-seq_len)\n",
    "                \n",
    "                if dims == 1:\n",
    "                    X_class[i] = np.expand_dims(data[rand:rand+seq_len], axis=1)\n",
    "                else:\n",
    "                    X_class[i] = data[rand:rand+seq_len]\n",
    "                \n",
    "        else:\n",
    "            data = df.loc[df['Label'] == label][features].to_numpy()\n",
    "            \n",
    "            num_samples = len(data)//50\n",
    "            print(num_samples)\n",
    "            \n",
    "            X_class = np.zeros((num_samples, seq_len, dims), dtype=np.float32)\n",
    "            Y_class = np.full((num_samples, 1), idx, dtype=np.int64)\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                if dims == 1:\n",
    "                    X_class[i] = np.expand_dims(data[seq_len*i:seq_len*(i+1)], axis=1)\n",
    "                else:\n",
    "                    X_class[i] = data[seq_len*i:seq_len*(i+1)]\n",
    "                    \n",
    "        X.append(X_class)\n",
    "        Y.append(Y_class)\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = sample_sequences(train_data, num_samples=10000, dims=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate(X_train)\n",
    "Y_train = np.concatenate(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, Y_train = shuffle(X_train, Y_train, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = sample_sequences(val_data, num_samples=2000, dims=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.concatenate(X_val)\n",
    "Y_val = np.concatenate(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = shuffle(X_val, Y_val, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train.mean())\n",
    "print(X_train.std())\n",
    "\n",
    "print(X_val.mean())\n",
    "print(X_val.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(np.squeeze(Y_train).tolist()))\n",
    "print(Counter(np.squeeze(Y_val).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = sample_sequences(test_data, train=False, dims=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate(X_test)\n",
    "Y_test = np.concatenate(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_odba['Label'].value_counts()//50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(test_odba['Label'].value_counts()//50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This estimates the distribution of each sequence (40,000/2,000 distributions, each of 50 samples)\n",
    "ax = sns.kdeplot(data=np.squeeze(X_train).T, legend=False)\n",
    "\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "ax = sns.kdeplot(data=np.squeeze(X_val).T, legend=False)\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This estimates the distribution of each sequence element (50 distributions, each of 40,000/2,000 samples)\n",
    "ax = sns.kdeplot(data=np.squeeze(X_train), legend=False)\n",
    "\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "ax = sns.kdeplot(data=np.squeeze(X_val), legend=False)\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This estimates the distribution over the train and val sets, respectively\n",
    "ax = sns.kdeplot(data=train_odba['ODBA'], legend=False)\n",
    "\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "ax = sns.kdeplot(data=val_odba['ODBA'], legend=False)\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(data=X_train.flatten(), legend=False)\n",
    "\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "ax = sns.kdeplot(data=X_val.flatten(), legend=False)\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(data, gts, outfile):\n",
    "    '''\n",
    "        This function writes the pre-processed image data to a HDF5 file\n",
    "        Args:\n",
    "          data: numpy.array, image data as numpy array\n",
    "          outfile: string, path to write file to\n",
    "    '''\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"Saving data\")\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    with h5py.File(outfile, \"w\") as f:\n",
    "        f.create_dataset(\"features\", data=data, dtype=data.dtype)\n",
    "        f.create_dataset(\"gts\", data=gts, dtype=gts.dtype)\n",
    "\n",
    "def load(infile, dataset):\n",
    "    '''\n",
    "        This function loads the image data from a HDF5 file \n",
    "        Args:\n",
    "          outfile: string, path to read file from\n",
    "          \n",
    "        Returns:\n",
    "          f[\"image\"][()]: numpy.array, image data as numpy array\n",
    "    '''\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"Loading data\")\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    with h5py.File(infile, \"r\") as f:\n",
    "        return f[dataset][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write(X_train, Y_train, './datasets/data/2d/train/data.hdf5')\n",
    "write(X_val, Y_val, './datasets/data/2d/val/data.hdf5')\n",
    "write(X_test, Y_test, './datasets/data/2d/test/orig/data.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
