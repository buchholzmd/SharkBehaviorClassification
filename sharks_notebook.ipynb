{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import h5py\n",
    "#import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/rnn/1d/LSTM\n",
      "./results/orig\n",
      "LSTM_1d_orig.txt\n"
     ]
    }
   ],
   "source": [
    "modelType = 'rnn'\n",
    "# modelType = 'cnn'\n",
    "# modelType = 'rcnn'\n",
    "\n",
    "expDim = '1d'\n",
    "# expDim = '2d'\n",
    "\n",
    "archType = 'LSTM'\n",
    "# archType = 'GRU'\n",
    "# archType = 'VGG1'\n",
    "# archType = 'VGG2'\n",
    "# archType = 'Inception'\n",
    "# archType = 'RCNN'\n",
    "\n",
    "testSplit = 'orig'\n",
    "# testSplit = '7000'\n",
    "# testSplit = '12000'\n",
    "\n",
    "wavelet = False\n",
    "\n",
    "folder_ = os.path.join('./models', modelType, expDim, archType)\n",
    "\n",
    "results_folder = os.path.join('./results', testSplit)\n",
    "\n",
    "if wavelet:\n",
    "    results_file = 'wavelet_' + archType + '_' + expDim + '_' + testSplit + '.txt'\n",
    "else:\n",
    "    results_file = archType + '_' + expDim + '_' + testSplit + '.txt'\n",
    "\n",
    "if not os.path.exists(folder_):\n",
    "    os.makedirs(folder_)\n",
    "    \n",
    "print(folder_)\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "    \n",
    "print(results_folder)\n",
    "print(results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all xml files in the categorical directory\n",
    "# NOTE: EXCEL COLUMN NAMES MUST BE CONSISTENT\n",
    "def get_data(dir):\n",
    "    df_train = pd.DataFrame()\n",
    "    df_val = pd.DataFrame()\n",
    "    df_test = pd.DataFrame()\n",
    "    for file in os.listdir(dir):\n",
    "        print(file)\n",
    "        df = pd.read_excel(dir + '/' + file, usecols=[1,2,3,4,5,6,7,8])\n",
    "        \n",
    "        print('total:', len(df), '60%: ', len(df)*.6, '20%: ',len(df)*.2)\n",
    "        \n",
    "        train = int(len(df)*.6)\n",
    "        val = int(train + len(df)*.2)\n",
    "        test = int(val + len(df)*.2)\n",
    "        df_train = df_train.append(df.iloc[:train])\n",
    "        df_val = df_val.append(df.iloc[train:val])\n",
    "        df_test = df_test.append(df.iloc[val:test])\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeding_25Hz_2.xlsx\n",
      "total: 350 60%:  210.0 20%:  70.0\n",
      "Feeding_25Hz_7.xlsx\n",
      "total: 2100 60%:  1260.0 20%:  420.0\n",
      "Feeding_25Hz_4.xlsx\n",
      "total: 1375 60%:  825.0 20%:  275.0\n",
      "Feeding_25Hz_3.xlsx\n",
      "total: 200 60%:  120.0 20%:  40.0\n",
      "Feeding_25Hz_5.xlsx\n",
      "total: 875 60%:  525.0 20%:  175.0\n",
      "Feeding_25Hz_1.xlsx\n",
      "total: 5700 60%:  3420.0 20%:  1140.0\n",
      "Feeding_25Hz_6.xlsx\n",
      "total: 2900 60%:  1740.0 20%:  580.0\n",
      "Resting_25Hz_4.xlsx\n",
      "total: 10250 60%:  6150.0 20%:  2050.0\n",
      "Resting_25Hz_5.xlsx\n",
      "total: 6150 60%:  3690.0 20%:  1230.0\n",
      "Resting_25Hz_6.xlsx\n",
      "total: 27975 60%:  16785.0 20%:  5595.0\n",
      "Resting_25Hz_3.xlsx\n",
      "total: 379850 60%:  227910.0 20%:  75970.0\n",
      "Resting_25Hz_2.xlsx\n",
      "total: 565580 60%:  339348.0 20%:  113116.0\n",
      "Resting_25Hz_7.xlsx\n",
      "total: 77374 60%:  46424.4 20%:  15474.800000000001\n",
      "Resting_25Hz_1.xlsx\n",
      "total: 157750 60%:  94650.0 20%:  31550.0\n",
      "Swimming_25Hz_7.xlsx\n",
      "total: 61475 60%:  36885.0 20%:  12295.0\n",
      "Swimming_25Hz_4.xlsx\n",
      "total: 81525 60%:  48915.0 20%:  16305.0\n",
      "Swimming_25Hz_6.xlsx\n",
      "total: 19750 60%:  11850.0 20%:  3950.0\n",
      "Swimming_25Hz_3.xlsx\n",
      "total: 5100 60%:  3060.0 20%:  1020.0\n",
      "Swimming_25Hz_1.xlsx\n",
      "total: 6200 60%:  3720.0 20%:  1240.0\n",
      "Swimming_25Hz_2.xlsx\n",
      "total: 209550 60%:  125730.0 20%:  41910.0\n",
      "Swimming_25Hz_5.xlsx\n",
      "total: 7975 60%:  4785.0 20%:  1595.0\n",
      "NDM_25Hz_1.xlsx\n",
      "total: 51950 60%:  31170.0 20%:  10390.0\n",
      "NDM_25Hz_7.xlsx\n",
      "total: 5400 60%:  3240.0 20%:  1080.0\n",
      "NDM_25Hz_4.xlsx\n",
      "total: 1750 60%:  1050.0 20%:  350.0\n",
      "NDM_25Hz_2.xlsx\n",
      "total: 7400 60%:  4440.0 20%:  1480.0\n",
      "NDM_25Hz_6.xlsx\n",
      "total: 27025 60%:  16215.0 20%:  5405.0\n",
      "NDM_25Hz_5.xlsx\n",
      "total: 1775 60%:  1065.0 20%:  355.0\n",
      "NDM_25Hz_3.xlsx\n",
      "total: 15700 60%:  9420.0 20%:  3140.0\n"
     ]
    }
   ],
   "source": [
    "all_data = './datasets/all_data'\n",
    "\n",
    "train_f, val_f, test_f = get_data(all_data + '/feeding/xlsx')\n",
    "train_r, val_r, test_r = get_data(all_data + '/resting/xlsx')\n",
    "train_s, val_s, test_s = get_data(all_data + '/swimming/xlsx')\n",
    "train_n, val_n, test_n = get_data(all_data + '/ndm/xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take the log of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if wavelet:\n",
    "    #train_f_cA, train_f_cD = pywt.dwt(train_f['ODBA'], 'db2')\n",
    "    logf = np.vstack(pywt.dwt(train_f['ODBA'], 'haar')).T\n",
    "    logr = np.vstack(pywt.dwt(train_r['ODBA'], 'haar')).T\n",
    "    logs = np.vstack(pywt.dwt(train_s['ODBA'], 'haar')).T\n",
    "    logn = np.vstack(pywt.dwt(train_n['ODBA'], 'haar')).T\n",
    "    \n",
    "    logvf = np.vstack(pywt.dwt(val_f['ODBA'], 'haar')).T\n",
    "    logvr = np.vstack(pywt.dwt(val_r['ODBA'], 'haar')).T\n",
    "    logvs = np.vstack(pywt.dwt(val_s['ODBA'], 'haar')).T\n",
    "    logvn = np.vstack(pywt.dwt(val_n['ODBA'], 'haar')).T\n",
    "    \n",
    "    logtf = np.vstack(pywt.dwt(test_f['ODBA'], 'haar')).T\n",
    "    logtr = np.vstack(pywt.dwt(test_r['ODBA'], 'haar')).T\n",
    "    logts = np.vstack(pywt.dwt(test_s['ODBA'], 'haar')).T\n",
    "    logtn = np.vstack(pywt.dwt(test_n['ODBA'], 'haar')).T\n",
    "    \n",
    "    plt.hist(train_f['ODBA'], bins='auto')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(pywt.dwt(train_f['ODBA'], 'haar')[0], bins='auto')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(pywt.dwt(train_f['ODBA'], 'haar')[1], bins='auto')\n",
    "    plt.show()\n",
    "    \n",
    "    print(train_f['ODBA'])\n",
    "    print(pywt.dwt(train_f['ODBA'], 'haar')[0])\n",
    "\n",
    "elif expDim is '1d':\n",
    "    logf= np.log10(train_f['ODBA'])\n",
    "    logr= np.log10(train_r['ODBA'])\n",
    "    logs= np.log10(train_s['ODBA'])\n",
    "    logn= np.log10(train_n['ODBA'])\n",
    "\n",
    "    logvf= np.log10(val_f['ODBA'])\n",
    "    logvr= np.log10(val_r['ODBA'])\n",
    "    logvs= np.log10(val_s['ODBA'])\n",
    "    logvn= np.log10(val_n['ODBA'])\n",
    "\n",
    "    logtf= np.log10(test_f['ODBA'])\n",
    "    logtr= np.log10(test_r['ODBA'])\n",
    "    logts= np.log10(test_s['ODBA'])\n",
    "    logtn= np.log10(test_n['ODBA'])\n",
    "    \n",
    "    plt.hist(train_f['ODBA'], bins='auto')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(logf, bins='auto')\n",
    "    plt.show()\n",
    "\n",
    "elif expDim is '2d':\n",
    "    logf= train_f[['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']].dropna().to_numpy()\n",
    "    logr= train_r[['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']].dropna().to_numpy()\n",
    "    logs= train_s[['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']].dropna().to_numpy()\n",
    "    logn= train_n[['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']].dropna().to_numpy()\n",
    "\n",
    "    logvf= val_f[['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']].dropna().to_numpy()\n",
    "    logvr= val_r[['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']].dropna().to_numpy()\n",
    "    logvs= val_s[['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']].dropna().to_numpy()\n",
    "    logvn= val_n[['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']].dropna().to_numpy()\n",
    "\n",
    "    logtf= test_f[['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']].dropna().to_numpy()\n",
    "    logtr= test_r[['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']].dropna().to_numpy()\n",
    "    logts= test_s[['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']].dropna().to_numpy()\n",
    "    logtn= test_n[['X_static', 'Y_static', 'Z_static', 'X_dynamic', 'Y_dynamic', 'Z_dynamic']].dropna().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(logf.isna().values.any())\n",
    "print(logr.isna().values.any())\n",
    "print(logs.isna().values.any())\n",
    "print(logn.isna().values.any())\n",
    "\n",
    "print(logvf.isna().values.any())\n",
    "print(logvr.isna().values.any())\n",
    "print(logvs.isna().values.any())\n",
    "print(logvn.isna().values.any())\n",
    "\n",
    "print(logtf.isna().values.any())\n",
    "print(logtr.isna().values.any())\n",
    "print(logts.isna().values.any())\n",
    "print(logtn.isna().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanf = np.mean(logf)\n",
    "stdf = np.std(logf)\n",
    "\n",
    "meanr = np.mean(logr)\n",
    "stdr = np.std(logr)\n",
    "\n",
    "means = np.mean(logs)\n",
    "stds = np.std(logs)\n",
    "\n",
    "meann = np.mean(logn)\n",
    "stdn = np.std(logn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ = (meanf+meanr+means+meann)/4\n",
    "std_ = (stdf+stdr+stds+stdn)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feed_X = (logf-mean_)/std_\n",
    "train_rest_X = (logr-mean_)/std_\n",
    "train_swim_X = (logs-mean_)/std_\n",
    "train_ndm_X = (logn-mean_)/std_\n",
    "\n",
    "val_feed_X = (logvf-mean_)/std_\n",
    "val_rest_X = (logvr-mean_)/std_\n",
    "val_swim_X = (logvs-mean_)/std_\n",
    "val_ndm_X = (logvn-mean_)/std_\n",
    "\n",
    "test_feed_X = (logtf-mean_)/std_\n",
    "test_rest_X = (logtr-mean_)/std_\n",
    "test_swim_X = (logts-mean_)/std_\n",
    "test_ndm_X = (logtn-mean_)/std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.concat([train_feed_X, train_rest_X, train_swim_X, train_ndm_X])\n",
    "val_all   = pd.concat([val_feed_X, val_rest_X, val_swim_X, val_ndm_X])\n",
    "test_all  = pd.concat([test_feed_X, test_rest_X, test_swim_X, test_ndm_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9177431197030054\n",
      "1.6721956196513221\n",
      "-0.8508600997144431\n",
      "1.6851847664806783\n",
      "-0.7312924840435562\n",
      "1.6287068177162298\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_all))\n",
    "print(np.std(train_all))\n",
    "\n",
    "print(np.mean(val_all))\n",
    "print(np.std(val_all))\n",
    "\n",
    "print(np.mean(test_all))\n",
    "print(np.std(test_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if expDim is '2d':\n",
    "    train_feed_X_nx = train_feed_X\n",
    "    train_swim_X_nx = train_swim_X\n",
    "    train_rest_X_nx = train_rest_X\n",
    "    train_ndm_X_nx = train_ndm_X\n",
    "\n",
    "    val_feed_X_nx = val_feed_X\n",
    "    val_swim_X_nx = val_swim_X\n",
    "    val_rest_X_nx = val_rest_X\n",
    "    val_ndm_X_nx = val_ndm_X\n",
    "\n",
    "    test_feed_X_nx = test_feed_X\n",
    "    test_swim_X_nx = test_swim_X\n",
    "    test_rest_X_nx = test_rest_X\n",
    "    test_ndm_X_nx = test_ndm_X\n",
    "\n",
    "else:\n",
    "    train_feed_X_nx = np.expand_dims(train_feed_X, axis=1)\n",
    "    train_swim_X_nx = np.expand_dims(train_swim_X, axis=1)\n",
    "    train_rest_X_nx = np.expand_dims(train_rest_X, axis=1)\n",
    "    train_ndm_X_nx = np.expand_dims(train_ndm_X, axis=1)\n",
    "\n",
    "    val_feed_X_nx = np.expand_dims(val_feed_X, axis=1)\n",
    "    val_swim_X_nx = np.expand_dims(val_swim_X, axis=1)\n",
    "    val_rest_X_nx = np.expand_dims(val_rest_X, axis=1)\n",
    "    val_ndm_X_nx = np.expand_dims(val_ndm_X, axis=1)\n",
    "\n",
    "    test_feed_X_nx = np.expand_dims(test_feed_X, axis=1)\n",
    "    test_swim_X_nx = np.expand_dims(test_swim_X, axis=1)\n",
    "    test_rest_X_nx = np.expand_dims(test_rest_X, axis=1)\n",
    "    test_ndm_X_nx = np.expand_dims(test_ndm_X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feed_X_nx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_swim_X_nx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rest_X_nx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ndm_X_nx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Samples\n",
    "* Train randomly sampled 10000 times in each category to balance data  \n",
    "* Validation randomly sampled 2000 times in each category\n",
    "* Test incrementally sampled    \n",
    "\n",
    "__Classification Labels:__  \n",
    "* Feeding = 0  \n",
    "* Resting = 1  \n",
    "* Swimming = 2\n",
    "* ndm = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If training, then randomly sample\n",
    "# If test, increment by 50 each time (sliding window non-overlapping)\n",
    "\n",
    "#dims is set to one to only take ODBM\n",
    "#for more data, expand dims\n",
    "\n",
    "# if expDim is '2d' and modelType is 'cnn':\n",
    "#     if wavelet:\n",
    "#         dims = 2\n",
    "#     else:\n",
    "#         dims = 6\n",
    "#     chan = 1\n",
    "#     def gen_samples(data, n_samples, label, n_frames, train=True):\n",
    "#         X = np.zeros((n_samples, chan, dims, n_frames), dtype=np.float32)\n",
    "#         Y = np.full((n_samples, 1), label, dtype=np.int64)\n",
    "#         print(data.shape)\n",
    "#         if train:\n",
    "#             for i in range(n_samples):\n",
    "#                 rand = np.random.randint(low=0, high=len(data)-n_frames)\n",
    "#                 for j in range(dims):\n",
    "#                     X[i][0][j] = data[rand:rand+n_frames, 0, j]\n",
    "#         else:\n",
    "#             for i in range(n_samples):\n",
    "#                 for j in range(dims):\n",
    "#                     X[i][0][j] = data[i*n_frames:i*n_frames+n_frames, 0, j]\n",
    "#         return X, Y\n",
    "    \n",
    "\n",
    "if wavelet:\n",
    "    dims = 2\n",
    "elif expDim is '1d':\n",
    "    dims = 1\n",
    "elif expDim is '2d':\n",
    "    dims = 6\n",
    "\n",
    "\n",
    "def gen_samples(data, n_samples, label, n_frames, train=True):\n",
    "    X = np.zeros((n_samples, dims, n_frames), dtype=np.float32)\n",
    "    Y = np.full((n_samples, 1), label, dtype=np.int64)\n",
    "    if train:\n",
    "        for i in range(0, n_samples):\n",
    "            rand = np.random.randint(low=0, high=len(data)-n_frames)\n",
    "            for j in range(0, dims):\n",
    "                X[i][j] = data[rand:rand+n_frames, j]\n",
    "    else:\n",
    "        for i in range(0, n_samples):\n",
    "            for j in range(0, dims):\n",
    "                X[i][j] = data[i*n_frames:i*n_frames+n_frames, j]\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54.0, 1566.3, 4899.7, 444.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2700/50), (78315/50), (244985/50), (22200/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate 10000 train samples in each category\n",
    "seed = 17\n",
    "n_frames = 50\n",
    "np.random.seed(seed)\n",
    "samples_train = 10000\n",
    "samples_val = 3000\n",
    "\n",
    "_train_feed_X, _train_feed_Y = gen_samples(train_feed_X_nx, samples_train, 0, n_frames, train=True)\n",
    "_train_rest_X, _train_rest_Y = gen_samples(train_rest_X_nx, samples_train, 1, n_frames, train=True)\n",
    "_train_swim_X, _train_swim_Y = gen_samples(train_swim_X_nx, samples_train, 2, n_frames, train=True)\n",
    "_train_ndm_X,  _train_ndm_Y  = gen_samples(train_ndm_X_nx, samples_train, 3, n_frames, train=True)\n",
    "\n",
    "train_X = np.concatenate((_train_feed_X, _train_rest_X, _train_swim_X, _train_ndm_X))\n",
    "train_Y = np.concatenate((_train_feed_Y, _train_rest_Y, _train_swim_Y, _train_ndm_Y))\n",
    "\n",
    "_val_feed_X, _val_feed_Y = gen_samples(val_feed_X_nx, samples_val, 0, n_frames, train=True)\n",
    "_val_rest_X, _val_rest_Y = gen_samples(val_rest_X_nx, samples_val, 1, n_frames, train=True)\n",
    "_val_swim_X, _val_swim_Y = gen_samples(val_swim_X_nx, samples_val, 2, n_frames, train=True)\n",
    "_val_ndm_X,  _val_ndm_Y  = gen_samples(val_ndm_X_nx, samples_val, 3, n_frames, train=True)\n",
    "\n",
    "val_X = np.concatenate((_val_feed_X, _val_rest_X, _val_swim_X, _val_ndm_X))\n",
    "val_Y = np.concatenate((_val_feed_Y, _val_rest_Y, _val_swim_Y, _val_ndm_Y))\n",
    "\n",
    "if testSplit is 'orig':\n",
    "    #number of samples generated from sliding window on 20% of data\n",
    "    feed__ = 54\n",
    "    rest__ = 4899\n",
    "    swim__ = 1566\n",
    "    ndm__ = 444\n",
    "#     feed__ = 54//2\n",
    "#     rest__ = 4899//2\n",
    "#     swim__ = 1566//2\n",
    "#     ndm__ = 444//2\n",
    "        \n",
    "    _test_feed_X, _test_feed_Y = gen_samples(test_feed_X_nx, feed__, 0, n_frames, train=False)\n",
    "    _test_rest_X, _test_rest_Y = gen_samples(test_rest_X_nx, rest__, 1, n_frames, train=False)\n",
    "    _test_swim_X, _test_swim_Y = gen_samples(test_swim_X_nx, swim__, 2, n_frames, train=False)\n",
    "    _test_ndm_X,  _test_ndm_Y  = gen_samples(test_ndm_X_nx, ndm__, 3, n_frames, train=False)\n",
    "\n",
    "elif testSplit is '7000':\n",
    "    #create random samples for testing - new test nums....\n",
    "    feed__ = 500\n",
    "    rest__ = 4000\n",
    "    swim__ = 2000\n",
    "    ndm__ = 500\n",
    "\n",
    "    _test_feed_X, _test_feed_Y = gen_samples(test_feed_X_nx, feed__, 0, n_frames, train=True)\n",
    "    _test_rest_X, _test_rest_Y = gen_samples(test_rest_X_nx, rest__, 1, n_frames, train=True)\n",
    "    _test_swim_X, _test_swim_Y = gen_samples(test_swim_X_nx, swim__, 2, n_frames, train=True)\n",
    "    _test_ndm_X,  _test_ndm_Y  = gen_samples(test_ndm_X_nx, ndm__, 3, n_frames, train=True)\n",
    "\n",
    "elif testSplit is '12000':\n",
    "    samples_test = 3000\n",
    "    \n",
    "    #create random samples for testing\n",
    "    _test_feed_X, _test_feed_Y = gen_samples(test_feed_X_nx, samples_test, 0, n_frames, train=True)\n",
    "    _test_rest_X, _test_rest_Y = gen_samples(test_rest_X_nx, samples_test, 1, n_frames, train=True)\n",
    "    _test_swim_X, _test_swim_Y = gen_samples(test_swim_X_nx, samples_test, 2, n_frames, train=True)\n",
    "    _test_ndm_X,  _test_ndm_Y  = gen_samples(test_ndm_X_nx, samples_test, 3, n_frames, train=True)\n",
    "    \n",
    "test_X = np.concatenate((_test_feed_X, _test_rest_X, _test_swim_X, _test_ndm_X))\n",
    "test_Y = np.concatenate((_test_feed_Y, _test_rest_Y, _test_swim_Y, _test_ndm_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/load augmented datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(data, outfile):\n",
    "    '''\n",
    "        This function writes the pre-processed image data to a HDF5 file\n",
    "        Args:\n",
    "          data: numpy.array, image data as numpy array\n",
    "          outfile: string, path to write file to\n",
    "    '''\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"Saving data\")\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    with h5py.File(outfile, \"w\") as f:\n",
    "        f.create_dataset(\"shark_data\", data=data, dtype=data.dtype)\n",
    "\n",
    "def load(infile):\n",
    "    '''\n",
    "        This function loads the image data from a HDF5 file \n",
    "        Args:\n",
    "          outfile: string, path to read file from\n",
    "          \n",
    "        Returns:\n",
    "          f[\"image\"][()]: numpy.array, image data as numpy array\n",
    "    '''\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"Loading data\")\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    with h5py.File(infile, \"r\") as f:\n",
    "        return f[\"shark_data\"][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.transpose((0,2,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join('./datasets/data', expDim)\n",
    "    \n",
    "if not os.path.exists(data_folder + '/' + testSplit):\n",
    "    os.makedirs(data_folder + '/' + testSplit)\n",
    "\n",
    "load_data = False\n",
    "\n",
    "if load_data:\n",
    "    train_X = load(data_folder + '/train_data.hdf5')\n",
    "    train_Y = load(data_folder + '/train_gt.hdf5')\n",
    "    val_X   = load(data_folder + '/val_data.hdf5')\n",
    "    val_Y   = load(data_folder + '/val_gt.hdf5')\n",
    "    test_X  = load(data_folder + '/' + testSplit + '/test_data.hdf5')\n",
    "    test_Y  = load(data_folder + '/' + testSplit + '/test_gt.hdf5')\n",
    "\n",
    "else:\n",
    "    write(train_X.transpose((0,2,1)), data_folder + '/train_data.hdf5')\n",
    "    write(train_Y, data_folder + '/train_gt.hdf5')\n",
    "    write(val_X.transpose((0,2,1)),   data_folder + '/val_data.hdf5')\n",
    "    write(val_Y, data_folder + '/val_gt.hdf5')\n",
    "    write(test_X.transpose((0,2,1)), data_folder + '/' + testSplit + '/test_data.hdf5')\n",
    "    write(test_Y, data_folder + '/' + testSplit + '/test_gt.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = (modelType is 'cnn') or (modelType is 'rcnn')\n",
    "\n",
    "if cnn and expDim is '2d':\n",
    "    train_X = np.expand_dims(train_X, axis=1)\n",
    "    val_X = np.expand_dims(val_X, axis=1)\n",
    "    test_X = np.expand_dims(test_X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X[333])\n",
    "print(val_X[123])\n",
    "print(train_Y[444])\n",
    "print(val_Y[321])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check?\n",
    "print(_train_feed_X.shape, _train_ndm_X.shape, _train_rest_X.shape, _train_swim_X.shape)\n",
    "print(_train_feed_Y.shape, _train_ndm_Y.shape, _train_rest_Y.shape, _train_swim_Y.shape)\n",
    "print(_train_feed_Y[5000], _train_ndm_Y[5000], _train_rest_Y[5000], _train_swim_Y[5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X.shape, val_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape, val_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharkBehaviorDataset(Dataset):\n",
    "    def __init__(self, data, labels=None, train=True):\n",
    "        self.data = data\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = None\n",
    "        if self.train:\n",
    "            sample_label = self.labels[idx]\n",
    "            return (sample, sample_label)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SharkBehaviorDataset(train_X, labels=train_Y, train=True)\n",
    "val_dataset = SharkBehaviorDataset(val_X, labels=val_Y, train=True)\n",
    "test_dataset = SharkBehaviorDataset(test_X, labels=test_Y, train=True)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from networks.cnn import SharkVGG1, SharkVGG2, Sharkception\n",
    "from networks.rnn import SharkLSTM, SharkGRU\n",
    "\n",
    "from networks.cnn2d import Shark2dVGG1, Shark2dVGG2, Sharkception2d \n",
    "\n",
    "from networks.rcnn import SharkRCNN2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelType is 'cnn':\n",
    "    if expDim is '1d':\n",
    "        if archType is 'VGG1':\n",
    "            model = SharkVGG1(1)\n",
    "        elif archType is 'VGG2':\n",
    "            model = SharkVGG2(1)\n",
    "        elif archType is 'Inception':\n",
    "            model = Sharkception(1)\n",
    "    \n",
    "    elif expDim is '2d':\n",
    "        if archType is 'VGG1':\n",
    "            model = Shark2dVGG1(1)\n",
    "        elif archType is 'VGG2':\n",
    "            model = Shark2dVGG2(1)\n",
    "        elif archType is 'Inception':\n",
    "            model = Sharkception2d(1)\n",
    "            \n",
    "    model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=.0)#, momentum=.9, nesterov=True)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=.99, nesterov=True)\n",
    "    sched = torch.optim.lr_scheduler.MultiStepLR(optimizer, [10000], gamma=0.1)\n",
    "    \n",
    "elif modelType is 'rnn':\n",
    "    hidden_size = 128\n",
    "    dat_size = 50\n",
    "    out_size = 4\n",
    "    \n",
    "    num_layers = 5\n",
    "    \n",
    "    if archType is 'LSTM':\n",
    "        model = SharkLSTM(dat_size, \n",
    "                          hidden_size, \n",
    "                          out_size, \n",
    "                          num_layers=num_layers)\n",
    "        \n",
    "    elif archType is 'GRU':\n",
    "        model = SharkGRU(dat_size, \n",
    "                         hidden_size, \n",
    "                         out_size, \n",
    "                         num_layers=num_layers)\n",
    "        \n",
    "    model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=.99, nesterov=True)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    sched = torch.optim.lr_scheduler.MultiStepLR(optimizer, [10000,20000], gamma=0.1)\n",
    "    \n",
    "if modelType is 'rcnn':\n",
    "    if expDim is '1d':\n",
    "        model = SharkRCNN(1, 4)\n",
    "    \n",
    "    elif expDim is '2d':\n",
    "        model = SharkRCNN2d(1, 4)\n",
    "            \n",
    "    model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=.0)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=.99, nesterov=True)\n",
    "    sched = torch.optim.lr_scheduler.MultiStepLR(optimizer, [10000], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    _, pred = torch.max(out,1)\n",
    "    correct = (pred == labels).sum().item()\n",
    "    acc = 100*correct/len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_train_losses = []\n",
    "mean_val_losses = []\n",
    "\n",
    "mean_train_acc = []\n",
    "mean_val_acc = []\n",
    "minLoss = 99999\n",
    "maxValacc = -99999\n",
    "for epoch in range(500):\n",
    "    print('EPOCH: ',epoch+1)\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    count = 0\n",
    "    for images, labels in train_loader:\n",
    "        labels = labels.squeeze()\n",
    "        images = Variable(images.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        train_acc.append(accuracy(outputs, labels))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        count +=1\n",
    "        \n",
    "    sched.step()\n",
    "    print('Training loss:.......', running_loss/count)\n",
    "    mean_train_losses.append(running_loss/count)\n",
    "        \n",
    "    model.eval()\n",
    "    count = 0\n",
    "    val_running_loss = 0.0\n",
    "    for images, labels in val_loader:\n",
    "        labels = labels.squeeze()\n",
    "        images = Variable(images.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "        \n",
    "        outputs = model(images)\n",
    "        accuracy(outputs, labels)\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        val_acc.append(accuracy(outputs, labels))\n",
    "        val_running_loss += loss.item()\n",
    "        count +=1\n",
    "\n",
    "    mean_val_loss = val_running_loss/count\n",
    "    print('Validation loss:.....', mean_val_loss)\n",
    "    \n",
    "    print('Training accuracy:...', np.mean(train_acc))\n",
    "    print('Validation accuracy..', np.mean(val_acc))\n",
    "    \n",
    "    mean_val_losses.append(mean_val_loss)\n",
    "    \n",
    "    mean_train_acc.append(np.mean(train_acc))\n",
    "    \n",
    "    val_acc_ = np.mean(val_acc)\n",
    "    mean_val_acc.append(val_acc_)\n",
    "    \n",
    "    if mean_val_loss < minLoss:\n",
    "        torch.save(model.state_dict(), './'+folder_+'/_loss.pth' )\n",
    "        print(f'NEW BEST LOSS_: {mean_val_loss} ........old best:{minLoss}')\n",
    "        minLoss = mean_val_loss\n",
    "        print('')\n",
    "        \n",
    "    if val_acc_ > maxValacc:\n",
    "        torch.save(model.state_dict(), './'+folder_+'/_acc.pth' )\n",
    "        print(f'NEW BEST ACC_: {val_acc_} ........old best:{maxValacc}')\n",
    "        maxValacc = val_acc_\n",
    "        \n",
    "    if epoch%500 == 0 :\n",
    "        torch.save(model.state_dict(), './'+folder_+'/save_'+str(epoch)+'.pth' )\n",
    "        print(f'DIV 200: Val_acc: {val_acc_} ..Val_loss:{mean_val_loss}')\n",
    "        \n",
    "    torch.save(model.state_dict(), './'+folder_+'/_last.pth' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_series = pd.Series(mean_train_acc)\n",
    "val_acc_series = pd.Series(mean_val_acc)\n",
    "train_acc_series.plot(label=\"train\")\n",
    "val_acc_series.plot(label=\"validation\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_acc_series = pd.Series(mean_train_losses)\n",
    "val_acc_series = pd.Series(mean_val_losses)\n",
    "train_acc_series.plot(label=\"train\")\n",
    "val_acc_series.plot(label=\"validation\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if modelType is 'cnn':\n",
    "    if expDim is '1d':\n",
    "        if archType is 'VGG1':\n",
    "            model = SharkVGG1(1)\n",
    "        elif archType is 'VGG2':\n",
    "            model = SharkVGG2(1)\n",
    "        elif archType is 'Inception':\n",
    "            model = Sharkception(1)\n",
    "    \n",
    "    elif expDim is '2d':\n",
    "        if archType is 'VGG1':\n",
    "            model = Shark2dVGG1(1)\n",
    "        elif archType is 'VGG2':\n",
    "            model = Shark2dVGG2(1)\n",
    "        elif archType is 'Inception':\n",
    "            model = Sharkception2d(1)\n",
    "            \n",
    "elif modelType is 'rnn':\n",
    "    hidden_size = 128\n",
    "    dat_size = 50\n",
    "    out_size = 4\n",
    "    \n",
    "    if archType is 'LSTM':\n",
    "        model = SharkLSTM(dat_size, hidden_size, out_size)\n",
    "    elif archType is 'GRU':\n",
    "        model = SharkGRU(dat_size, hidden_size, out_size)\n",
    "        \n",
    "elif modelType is 'rcnn':\n",
    "    \n",
    "    if expDim is '1d':\n",
    "        model = SharkRCNN(1, 4)\n",
    "    \n",
    "    elif expDim is '2d':\n",
    "        model = SharkRCNN2d(1, 4)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "state_dict = torch.load('./'+ folder_ + '/_acc.pth')\n",
    "# state_dict = torch.load('exp/'+ folder_ +'/_loss.pth')\n",
    "# state_dict = torch.load('exp/'+ folder_ +'/_last.pth')\n",
    "# state_dict = torch.load('exp/'+ folder_ +'/save_1000.pth') #0,200...\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "ys = []\n",
    "probs = np.empty((test_X.shape[0],4))\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval\n",
    "    count = 0\n",
    "#     preds = []\n",
    "#     ys = []\n",
    "    val_running_loss = 0.0\n",
    "    for images, labels in test_loader:\n",
    "#         tots +=1\n",
    "        labels = labels.squeeze()\n",
    "        images = Variable(images.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "        \n",
    "        outputs = model(images)\n",
    "        prob = nn.Softmax()(outputs).cpu().numpy()\n",
    "        \n",
    "        _, pred = torch.max(outputs,1)\n",
    "        ys = np.concatenate([ys, labels.cpu().numpy()])\n",
    "        preds = np.concatenate([preds, pred.cpu().numpy()])\n",
    "        \n",
    "        #probs.append(prob.tolist())\n",
    "        #print(prob.shape)\n",
    "        probs = np.vstack([probs, prob])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy --8000\n",
    "print('Accuracy: ', (ys == preds).sum() / ys.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(ys, preds, [0,1,2,3])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(ys, preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minLoss, maxValacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(probs)\n",
    "probs1 = probs[test_X.shape[0]:]\n",
    "\n",
    "print(test_X.shape[0])\n",
    "print(probs1.shape)\n",
    "\n",
    "print(metrics.roc_auc_score(ys, probs1, multi_class='ovo', labels=[0,1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(results_folder, results_file), 'w+') as f:\n",
    "    f.write(\"Test accuracy\\n\")\n",
    "    f.write(\"----------------------------------\\n\")\n",
    "    f.write(str((ys == preds).sum() / ys.shape[0]))\n",
    "    f.write('\\n\\n')\n",
    "    f.write(\"Confusion matrix\\n\")\n",
    "    f.write(\"----------------------------------\\n\")\n",
    "    f.write(str(confusion_matrix(ys, preds, [0,1,2,3])))\n",
    "    f.write('\\n\\n')\n",
    "    f.write(\"Classification report\\n\")\n",
    "    f.write(\"----------------------------------\\n\")\n",
    "    f.write(str(classification_report(ys, preds, digits=4)))\n",
    "    f.write('\\n\\n')\n",
    "    f.write(\"AUC ROC\\n\")\n",
    "    f.write(\"----------------------------------\\n\")\n",
    "    f.write(str(metrics.roc_auc_score(ys, probs1, multi_class='ovo', labels=[0,1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
